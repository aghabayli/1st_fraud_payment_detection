{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1st_fraud_payment_detection.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aghabayli/1st_fraud_payment_detection/blob/master/1st_fraud_payment_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DhDZCrqjf6r",
        "colab_type": "code",
        "outputId": "7846f7c3-2ed1-481f-9c11-c4c243b8684e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Connect to the drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBeZueBprNhu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import preprocessing, tree, metrics\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_curve, auc\n",
        "import statsmodels.api as sm\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIy630BFrtPw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load dataset\n",
        "payments = pd.read_csv('gdrive/My Drive/1st_adyen_rides-success-and-fail.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62qdR6nUuuXS",
        "colab_type": "text"
      },
      "source": [
        "# Data Exploration\n",
        "\n",
        "First, we will clarify the following: which features we have, which are relevant, what type of they are, how balanced data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Widc9PWLusMB",
        "colab_type": "code",
        "outputId": "160e5e74-c4ad-4211-8927-7df857b8e77d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        }
      },
      "source": [
        "#Examples from the data set\n",
        "payments.head()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>created</th>\n",
              "      <th>device_name</th>\n",
              "      <th>device_os_version</th>\n",
              "      <th>country</th>\n",
              "      <th>city_id</th>\n",
              "      <th>lat</th>\n",
              "      <th>lng</th>\n",
              "      <th>real_destination_lat</th>\n",
              "      <th>real_destination_lng</th>\n",
              "      <th>user_id</th>\n",
              "      <th>order_id</th>\n",
              "      <th>order_try_id</th>\n",
              "      <th>distance</th>\n",
              "      <th>ride_distance</th>\n",
              "      <th>price</th>\n",
              "      <th>ride_price</th>\n",
              "      <th>price_review_status</th>\n",
              "      <th>price_review_reason</th>\n",
              "      <th>is_successful_payment</th>\n",
              "      <th>name</th>\n",
              "      <th>card_bin</th>\n",
              "      <th>failed_attempts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2016-01-23 23:10:07</td>\n",
              "      <td>motorolaXT1562</td>\n",
              "      <td>motorola6.0.1</td>\n",
              "      <td>ee</td>\n",
              "      <td>2.0</td>\n",
              "      <td>58.378220</td>\n",
              "      <td>26.710402</td>\n",
              "      <td>58.363243</td>\n",
              "      <td>26.737696</td>\n",
              "      <td>218</td>\n",
              "      <td>4047728</td>\n",
              "      <td>4054895</td>\n",
              "      <td>773</td>\n",
              "      <td>3017</td>\n",
              "      <td>4.5000</td>\n",
              "      <td>4.5000</td>\n",
              "      <td>ok</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>**** 0810</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2016-05-04 06:01:32</td>\n",
              "      <td>iPhone6</td>\n",
              "      <td>iOS10.3.3</td>\n",
              "      <td>ee</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.424130</td>\n",
              "      <td>24.646359</td>\n",
              "      <td>59.397548</td>\n",
              "      <td>24.660957</td>\n",
              "      <td>266</td>\n",
              "      <td>5093642</td>\n",
              "      <td>5129745</td>\n",
              "      <td>43</td>\n",
              "      <td>4241</td>\n",
              "      <td>4.4000</td>\n",
              "      <td>4.4000</td>\n",
              "      <td>ok</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>**** 9115</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2016-08-27 16:42:22</td>\n",
              "      <td>HTCHTC 10</td>\n",
              "      <td>HTC7.0</td>\n",
              "      <td>ee</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.413508</td>\n",
              "      <td>24.743706</td>\n",
              "      <td>59.448500</td>\n",
              "      <td>24.804887</td>\n",
              "      <td>551</td>\n",
              "      <td>6655300</td>\n",
              "      <td>6792534</td>\n",
              "      <td>1654</td>\n",
              "      <td>6347</td>\n",
              "      <td>7.2000</td>\n",
              "      <td>7.2000</td>\n",
              "      <td>ok</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>**** 0634</td>\n",
              "      <td>516903.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2016-10-25 07:14:27</td>\n",
              "      <td>iPhone6S</td>\n",
              "      <td>iOS10.3.2</td>\n",
              "      <td>ee</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.419938</td>\n",
              "      <td>24.744795</td>\n",
              "      <td>59.431686</td>\n",
              "      <td>24.720801</td>\n",
              "      <td>798</td>\n",
              "      <td>7874827</td>\n",
              "      <td>8103655</td>\n",
              "      <td>883</td>\n",
              "      <td>2638</td>\n",
              "      <td>3.1000</td>\n",
              "      <td>3.1000</td>\n",
              "      <td>ok</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>**** 8730</td>\n",
              "      <td>541747.0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2016-09-09 12:46:47</td>\n",
              "      <td>iPhone5,2</td>\n",
              "      <td>iOS9.3.4</td>\n",
              "      <td>ee</td>\n",
              "      <td>1.0</td>\n",
              "      <td>59.471328</td>\n",
              "      <td>24.890557</td>\n",
              "      <td>59.427836</td>\n",
              "      <td>24.774460</td>\n",
              "      <td>944</td>\n",
              "      <td>6879043</td>\n",
              "      <td>7039724</td>\n",
              "      <td>1109</td>\n",
              "      <td>10288</td>\n",
              "      <td>9.0999</td>\n",
              "      <td>9.0999</td>\n",
              "      <td>ok</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>**** 3503</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               created     device_name  ...  card_bin failed_attempts\n",
              "0  2016-01-23 23:10:07  motorolaXT1562  ...       NaN               0\n",
              "1  2016-05-04 06:01:32         iPhone6  ...       NaN               0\n",
              "2  2016-08-27 16:42:22       HTCHTC 10  ...  516903.0               0\n",
              "3  2016-10-25 07:14:27        iPhone6S  ...  541747.0               0\n",
              "4  2016-09-09 12:46:47       iPhone5,2  ...       NaN               0\n",
              "\n",
              "[5 rows x 22 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VXvGYac9hyFW",
        "colab_type": "text"
      },
      "source": [
        "I tried to construct timestamp, in a way that the repetitions are possible. Extracted year, month, day, hour, weekday as a separate column. After testing the results for initial and reconstructed, I did not see improvements, so did not include to the final version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbJ2fgTNdyRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Constact from date week number\n",
        "#payments = payments.assign(month = payments['created'].apply(lambda x : pd.to_datetime(x).week))\n",
        "\n",
        "#Constact from date week number\n",
        "#payments = payments.assign(year = payments['created'].apply(lambda x : pd.to_datetime(x).week))\n",
        "\n",
        "#Constact from date week number\n",
        "#payments = payments.assign(week = payments['created'].apply(lambda x: pd.to_datetime(x).week))\n",
        "\n",
        "#Constact from date hours\n",
        "#payments = payments.assign(hour = payments['created'].apply(lambda x: pd.to_datetime(x).hour))\n",
        "\n",
        "#Constact from date weekdays\n",
        "#payments = payments.assign(weekday = payments['created'].apply(lambda x: pd.to_datetime(x).weekday()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAUe78hWu2Tx",
        "colab_type": "code",
        "outputId": "11bd8c13-e593-48c4-eb00-4bf9bfa94149",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "#Information about fields\n",
        "payments.info()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 304053 entries, 0 to 304052\n",
            "Data columns (total 22 columns):\n",
            "created                  304053 non-null object\n",
            "device_name              304053 non-null object\n",
            "device_os_version        304053 non-null object\n",
            "country                  304052 non-null object\n",
            "city_id                  303734 non-null float64\n",
            "lat                      304053 non-null float64\n",
            "lng                      304053 non-null float64\n",
            "real_destination_lat     304026 non-null float64\n",
            "real_destination_lng     304026 non-null float64\n",
            "user_id                  304053 non-null int64\n",
            "order_id                 304053 non-null int64\n",
            "order_try_id             304053 non-null int64\n",
            "distance                 304053 non-null int64\n",
            "ride_distance            304053 non-null int64\n",
            "price                    304053 non-null float64\n",
            "ride_price               304053 non-null float64\n",
            "price_review_status      304053 non-null object\n",
            "price_review_reason      1108 non-null object\n",
            "is_successful_payment    304053 non-null int64\n",
            "name                     304053 non-null object\n",
            "card_bin                 297475 non-null float64\n",
            "failed_attempts          304053 non-null int64\n",
            "dtypes: float64(8), int64(7), object(7)\n",
            "memory usage: 51.0+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uspuJ38hxYVQ",
        "colab_type": "code",
        "outputId": "13a0d6d3-7c26-4271-b4a8-0d912fa08399",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "#Procent of fraudelent transactions (minority class)\n",
        "print(\"Precent of fraudulent transactions: \")\n",
        "payments.is_successful_payment.value_counts()[0]/(payments.is_successful_payment.value_counts()[0]+payments.is_successful_payment.value_counts()[1])"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precent of fraudulent transactions: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25993165665196527"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kC1Mxlaxbnlg",
        "colab_type": "text"
      },
      "source": [
        "We can see that our data set is unbalanced, as contains 25 percent fraudulent and 75 normal transactions. Usually, 40%-60%, 50%-50% are considered as balanced. First, I will work with original data, then balance and try again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8JhscsRiQa5",
        "colab_type": "text"
      },
      "source": [
        "The features with unique values are irrelavant in our case. We constructed the labels vector and removed the following columns: 'created', 'user_id', 'order_id', 'order_try_id', 'name', 'card_bin', 'is_successful_payment' from input data. 'user_id', 'order_id' and 'order_try_id' are unique. As a usage of the identical card fraudelent transaction were handled before, we removed 'name' and 'card_bin'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vbx8Byn02JE-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Splitting data into labels and features data\n",
        "\n",
        "y = payments.is_successful_payment\n",
        "\n",
        "#Delete unique features\n",
        "X = payments.drop(['created', 'user_id', 'order_id', 'order_try_id', 'name', 'card_bin', 'is_successful_payment'], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGfHlvd8jSxk",
        "colab_type": "text"
      },
      "source": [
        "Then, we encode categorical values to labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzolPVFS28Rd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Encoding to labels features contained string values\n",
        "encode = preprocessing.LabelEncoder()\n",
        "X['device_name'] = encode.fit_transform(X.device_name)\n",
        "X['device_os_version'] = encode.fit_transform(X.device_os_version)\n",
        "X['country'] = encode.fit_transform(X.country.fillna('nan'))\n",
        "X['price_review_status'] = encode.fit_transform(X.price_review_status)\n",
        "X['price_review_reason'] = encode.fit_transform(X.price_review_reason.fillna('nan'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpIQdDC9jpty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "f3c353ab-fa92-4747-9334-448ef0f0e016"
      },
      "source": [
        "#Checking for missing values\n",
        "X.isnull().sum()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device_name               0\n",
              "device_os_version         0\n",
              "country                   0\n",
              "city_id                 319\n",
              "lat                       0\n",
              "lng                       0\n",
              "real_destination_lat     27\n",
              "real_destination_lng     27\n",
              "distance                  0\n",
              "ride_distance             0\n",
              "price                     0\n",
              "ride_price                0\n",
              "price_review_status       0\n",
              "price_review_reason       0\n",
              "failed_attempts           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYsN1taije9O",
        "colab_type": "text"
      },
      "source": [
        "There are missing values in city id, real destination latitude, and longitude. All remaining missing values were filled with zero."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYQCCoal3Xqr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Fill with zero missing values\n",
        "X = X.fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9rdQLOXkgpO",
        "colab_type": "text"
      },
      "source": [
        "We split data set to train and test as 70% and 30%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nihDNXvx3iuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split into train and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2utjWjH4e-f",
        "colab_type": "text"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e-waFG5okoMm",
        "colab_type": "text"
      },
      "source": [
        "I decided to start the training process with logistic regression, to be able to see how useful each feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MOso0HP4buGp",
        "colab_type": "code",
        "outputId": "60634527-8153-44ae-f35d-7b5cf0bfd25e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "#Using Logit to select features\n",
        "logit_model=sm.Logit(y_train,X_train)\n",
        "result=logit_model.fit()\n",
        "print(result.summary2())"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: 0.519478\n",
            "         Iterations 7\n",
            "                            Results: Logit\n",
            "=======================================================================\n",
            "Model:              Logit                 Pseudo R-squared: 0.093      \n",
            "Dependent Variable: is_successful_payment AIC:              221158.3776\n",
            "Date:               2019-07-07 08:38      BIC:              221312.4018\n",
            "No. Observations:   212837                Log-Likelihood:   -1.1056e+05\n",
            "Df Model:           14                    LL-Null:          -1.2189e+05\n",
            "Df Residuals:       212822                LLR p-value:      0.0000     \n",
            "Converged:          1.0000                Scale:            1.0000     \n",
            "No. Iterations:     7.0000                                             \n",
            "-----------------------------------------------------------------------\n",
            "                        Coef.  Std.Err.    z     P>|z|   [0.025  0.975]\n",
            "-----------------------------------------------------------------------\n",
            "device_name             0.0001   0.0000   1.8364 0.0663 -0.0000  0.0001\n",
            "device_os_version       0.0000   0.0001   0.1118 0.9110 -0.0002  0.0002\n",
            "country                -0.0480   0.0011 -43.0603 0.0000 -0.0502 -0.0459\n",
            "city_id                -0.0028   0.0000 -63.8515 0.0000 -0.0029 -0.0027\n",
            "lat                    -0.0068   0.0126  -0.5416 0.5881 -0.0316  0.0179\n",
            "lng                     0.0134   0.0205   0.6538 0.5132 -0.0267  0.0535\n",
            "real_destination_lat    0.0036   0.0126   0.2879 0.7734 -0.0211  0.0284\n",
            "real_destination_lng   -0.0060   0.0205  -0.2925 0.7699 -0.0461  0.0341\n",
            "distance               -0.0000   0.0000  -5.9551 0.0000 -0.0000 -0.0000\n",
            "ride_distance           0.0000   0.0000  22.1397 0.0000  0.0000  0.0000\n",
            "price                   0.0047   0.0001  57.1893 0.0000  0.0046  0.0049\n",
            "ride_price             -0.0042   0.0001 -52.9250 0.0000 -0.0043 -0.0040\n",
            "price_review_status     2.6823   0.0583  46.0016 0.0000  2.5680  2.7966\n",
            "price_review_reason    -1.0002   0.0526 -19.0219 0.0000 -1.1033 -0.8971\n",
            "failed_attempts        -0.1070   0.0048 -22.4729 0.0000 -0.1164 -0.0977\n",
            "=======================================================================\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "obJ_5SXzl2X6",
        "colab_type": "text"
      },
      "source": [
        "The features with higher than 0.05 p values are less significant features. Device os version, latitude and longitude, and device name information are less significant. The reason can be as well need for more feature engineering. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyzZLku27n5w",
        "colab_type": "text"
      },
      "source": [
        "The next I excluded less significant features and trained logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YC_faB4-dKCZ",
        "colab_type": "code",
        "outputId": "b0f52e23-b993-40bc-ba63-61314d9dd444",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "source": [
        "#Delete features with p>0.05\n",
        "X1 = X.drop(['device_name', 'device_os_version', 'lat', 'lng', 'real_destination_lat', 'real_destination_lng', 'distance'], axis=1)\n",
        "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y, test_size=0.3, random_state=0)\n",
        "\n",
        "#Logistic regression\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "#Train logistic regression model\n",
        "logreg.fit(X_train1, y_train1)\n",
        "\n",
        "predicted = logreg.predict(X_test1)\n",
        "\n",
        "#Accuracy\n",
        "print(\"Accuracy for the test data: {}\".format(accuracy_score(y_test1, predicted)))\n",
        "\n",
        "#Calculate confusion matrix\n",
        "print(\"Confusion matrix for the test data: \\n{}\".format(confusion_matrix(y_test1, predicted)))\n",
        "\n",
        "#F measure\n",
        "print(\"F-measure for the test data: {}\".format(f1_score(y_test1, predicted)))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the test data: 0.7474017716190142\n",
            "Confusion matrix for the test data: \n",
            "[[ 1653 22118]\n",
            " [  923 66522]]\n",
            "F-measure for the test data: 0.8523817150911362\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0qyCodfDmiHc",
        "colab_type": "text"
      },
      "source": [
        "As both false positives and false negatives are important for the problem. We want to identify more frauds and at the same time do not bother honest users. I will use F-measure and accuracy to compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFcZrZDs8HQE",
        "colab_type": "text"
      },
      "source": [
        "We can see that, logistic regression showed not very good results. The number of false positives is high. Accuracy is 0.74, f-measure is 0.85."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVv7E3ImW0b1",
        "colab_type": "text"
      },
      "source": [
        "### Decision tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA5XZZwFnqZ0",
        "colab_type": "text"
      },
      "source": [
        "I decided to move with the Decision Tree algorithm,  which is more stable for unbalanced data sets and have a good fit for the problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LGiudDl3sO1",
        "colab_type": "code",
        "outputId": "f1dd4f08-01de-4517-a8c7-5fa55ef04184",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#Decision tree prediction\n",
        "clf = tree.DecisionTreeClassifier()\n",
        "\n",
        "#Apply cross validation\n",
        "scores = cross_val_score(clf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Accuracy for the validation data sets: {}\".format(scores.mean()))\n",
        "\n",
        "#Train decision tree model on all train data\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "#Predict labels of test data\n",
        "predicted = clf.predict(X_test)\n",
        "\n",
        "#Accuracy\n",
        "print(\"Accuracy for the test data: {}\".format(accuracy_score(y_test, predicted)))\n",
        "\n",
        "#Calculate confusion matrix\n",
        "print(\"Confusion matrix for the test data: \\n{}\".format(confusion_matrix(y_test, predicted)))\n",
        "\n",
        "#F measure\n",
        "print(\"F-measure for the test data: {}\".format(f1_score(y_test, predicted)))"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the validation data sets: 0.9124494235641395\n",
            "Accuracy for the test data: 0.9115725311348886\n",
            "Confusion matrix for the test data: \n",
            "[[19901  3870]\n",
            " [ 4196 63249]]\n",
            "F-measure for the test data: 0.9400582622395292\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwXOPOllqMF3",
        "colab_type": "text"
      },
      "source": [
        "The accuracy and f-measure results seem to be not bad. As decision tree algorithm worked well, as a next step I will try tree ensembling methods Random Forest and Ada Boost."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGGRwe-1W5JA",
        "colab_type": "text"
      },
      "source": [
        "### Random forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMlbrJO_kDyG",
        "colab_type": "code",
        "outputId": "1947a42b-5a50-408c-d387-da79c87e7a46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "#Create a model with 5 trees\n",
        "rf = RandomForestClassifier(n_estimators = 5, random_state = 0)\n",
        "\n",
        "#Apply cross validation\n",
        "scores = cross_val_score(rf, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Accuracy for the validation data sets: {}\".format(scores.mean()))\n",
        "\n",
        "#Train the model\n",
        "rf.fit(X_train,y_train)\n",
        "\n",
        "#Predict test data\n",
        "predicted = rf.predict(X_test)\n",
        "\n",
        "#Accuracy\n",
        "print(\"Accuracy for the test data: {}\".format(accuracy_score(y_test, predicted)))\n",
        "\n",
        "#Calculate confusion matrix\n",
        "print(\"Confusion matrix for the test data: \\n{}\".format(confusion_matrix(y_test, predicted)))\n",
        "\n",
        "#F measure\n",
        "print(\"F-measure for the test data: {}\".format(f1_score(y_test, predicted)))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the validation data sets: 0.9440604729671067\n",
            "Accuracy for the test data: 0.9428609015962112\n",
            "Confusion matrix for the test data: \n",
            "[[19607  4164]\n",
            " [ 1048 66397]]\n",
            "F-measure for the test data: 0.9622335260785763\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpEL_3YfrAh2",
        "colab_type": "text"
      },
      "source": [
        "We can see that, the results were improved the accuracy is 94% and f-measure is 0.96."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dF37h4kYFZM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "cc8dae83-1cdd-48a0-bce0-9b6835afc50e"
      },
      "source": [
        "#Ada Boost Classifier\n",
        "ada = AdaBoostClassifier(n_estimators=5, learning_rate=1)\n",
        "\n",
        "#Apply cross validation\n",
        "scores = cross_val_score(ada, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(\"Accuracy for the validation data sets: {}\".format(scores.mean()))\n",
        "\n",
        "#Train the model\n",
        "ada.fit(X_train,y_train)\n",
        "\n",
        "#Predict test data\n",
        "predicted = ada.predict(X_test)\n",
        "\n",
        "#Accuracy\n",
        "print(\"Accuracy for the test data: {}\".format(accuracy_score(y_test, predicted)))\n",
        "\n",
        "#Calculate confusion matrix\n",
        "print(\"Confusion matrix for the test data: \\n{}\".format(confusion_matrix(y_test, predicted)))\n",
        "\n",
        "#F measure\n",
        "print(\"F-measure for the test data: {}\".format(f1_score(y_test, predicted)))"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the validation data sets: 0.946513060761454\n",
            "Accuracy for the test data: 0.9445163129275566\n",
            "Confusion matrix for the test data: \n",
            "[[18750  5021]\n",
            " [   40 67405]]\n",
            "F-measure for the test data: 0.9638166596363792\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCrD02_peu95",
        "colab_type": "text"
      },
      "source": [
        "# Clustering Problem\n",
        "\n",
        "As a next method, I will implement the problem as if we are not able to label data set beforehand. I will look at the problem as an unsupervised and apply K Means method to group users to the 2 group considering similarity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3elrnKWeyqr",
        "colab_type": "code",
        "outputId": "2b712f9f-3d37-42d8-cfe1-309861278071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "#K Means\n",
        "kmeans = KMeans(n_clusters=2, random_state=0)\n",
        "\n",
        "#Train\n",
        "kmeans.fit(X_train)\n",
        "\n",
        "#Predict\n",
        "predicted = kmeans.predict(X_test)\n",
        "\n",
        "#Accuracy\n",
        "print(\"Accuracy for the test data: {}\".format(accuracy_score(y_test, predicted)))\n",
        "\n",
        "#Calculate confusion matrix\n",
        "print(\"Confusion matrix for the test data: \\n{}\".format(confusion_matrix(y_test, predicted)))\n",
        "\n",
        "#F measure\n",
        "print(\"F-measure for the test data: {}\".format(f1_score(y_test, predicted)))"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for the test data: 0.6741032275039467\n",
            "Confusion matrix for the test data: \n",
            "[[ 1804 21967]\n",
            " [ 7760 59685]]\n",
            "F-measure for the test data: 0.8006197307792914\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMAuBVNVt6Pi",
        "colab_type": "text"
      },
      "source": [
        "As expected, the results of the clustering method were lower than the classification. The accuracy of the model is 0.67 and f-measure 0.8. From the confusion matrix, we see that, the number of false positives (fraudulent ransaction predicted as an honest one) is high. The reason can unbalanced data or need for more feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDDNUW3Qz9Ci",
        "colab_type": "text"
      },
      "source": [
        "#Sampling Data \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnhuHzO3yIqs",
        "colab_type": "text"
      },
      "source": [
        "After all described before, I resampled the data and tried all algorithms with balanced data sets. There were 2 methods implemented to balance the data set: Randomly Oversampling and Synthetic Minority Over-sampling Technique. All methods did not show better results with balanced data sets. And as Tree based methods showed good results with unbalanced as well, I decided to report results from original data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZ0dJZEYOEqV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "smote = SMOTE(ratio='minority')\n",
        "X_sm, y_sm = smote.fit_sample(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZzTrTuXOD61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "ros = RandomOverSampler(random_state=0)\n",
        "X_ros, y_ros = ros.fit_sample(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T5Ptn5uh46GC",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "Random Forest and Ada Boost are the top 2 methods from tested. Comparing accuracy and F measures these methods showed better and stable results. "
      ]
    }
  ]
}